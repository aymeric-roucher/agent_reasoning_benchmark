{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain numexpr sentencepiece plotly openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chat_models import ChatHuggingFace\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from scripts.evaluation import load_benchmark\n",
    "import datasets\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/aymeric/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from transformers.agents import HfEngine\n",
    "from transformers.agents.llm_engine import (\n",
    "    get_clean_message_list,\n",
    "    llama_role_conversions,\n",
    ")\n",
    "\n",
    "command_r_id = \"CohereForAI/c4ai-command-r-plus\"\n",
    "zephyr_id = \"HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\"\n",
    "mixtral_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llama3_8b_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llama3_70b_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "mixtral_large_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=zephyr_id,\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=1024,\n",
    "#     do_sample=False,\n",
    "#     repetition_penalty=1.03,\n",
    "# )\n",
    "\n",
    "# llm_engine_hf = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "class OAIEngine:\n",
    "    def __init__(self):\n",
    "        self.client = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "    def __call__(self, messages, stop=[]) -> str:\n",
    "        # Get clean message list\n",
    "        messages = get_clean_message_list(\n",
    "            messages, role_conversions=llama_role_conversions\n",
    "        )\n",
    "\n",
    "        return self.client.invoke(messages, stop=stop).content\n",
    "\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "eval_model_name = \"GPT4\"\n",
    "\n",
    "\n",
    "# def call_llm_hf(input: str, stop=[\"Observation\", \"Final Answer\"]) -> str:\n",
    "#     return llm_engine_hf.invoke(input, stop=stop).content\n",
    "\n",
    "\n",
    "# call_llm_hf(\"Please output 'Observation'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL_ID = \"gpt-4-0125-preview\"\n",
    "OUTPUT_DIR = \"output_reasoning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = load_benchmark()\n",
    "eval_df[\"true_answer\"] = eval_df[\"true_answer\"].astype(str)\n",
    "eval_ds = datasets.Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tools\n",
    "To run the langchain SERPAPI tool, you will need to have a [SerpAPI](https://serpapi.com/dashboard) API key: for this you need a paid account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.agents.default_tools import PythonInterpreterTool, Tool\n",
    "from scripts.agents import CalculatorTool\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "params = {\n",
    "    \"engine\": \"bing\",\n",
    "    \"gl\": \"us\",\n",
    "    \"hl\": \"en\",\n",
    "}\n",
    "langchain_serpapi = SerpAPIWrapper(params=params)\n",
    "\n",
    "\n",
    "class SearchTool(Tool):\n",
    "    name = \"search\"\n",
    "    description = \"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\"\n",
    "\n",
    "    inputs = {\"query\": {\"description\": \"your search query\", \"type\": \"text\"}}\n",
    "    output_type = \"text\"\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        return langchain_serpapi.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLBOX_CODE = [SearchTool()]\n",
    "\n",
    "TOOLBOX_JSON = [SearchTool(), PythonInterpreterTool()]\n",
    "\n",
    "TOOLBOX_CALC = [SearchTool(), CalculatorTool()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/aymeric/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last message must be a HumanMessage!\n",
      "System prompt not supported\n"
     ]
    }
   ],
   "source": [
    "from transformers import CodeAgent, ReactCodeAgent, ReactJsonAgent\n",
    "\n",
    "from scripts.agents import build_hf_agent_with_tools, build_openai_agent_with_tools\n",
    "\n",
    "\n",
    "react_code_agent_openai = ReactCodeAgent(\n",
    "    llm_engine=OAIEngine(), tools=TOOLBOX_CODE, max_iterations=7\n",
    ")\n",
    "react_json_agent_openai = ReactJsonAgent(\n",
    "    llm_engine=OAIEngine(), tools=TOOLBOX_JSON, max_iterations=7\n",
    ")\n",
    "\n",
    "langchain_agent = build_hf_agent_with_tools(repo_id=llama3_70b_id)\n",
    "langchain_openai_agent = build_openai_agent_with_tools()\n",
    "\n",
    "react_json_agent_llama3_70 = ReactJsonAgent(\n",
    "    llm_engine=HfEngine(model=llama3_70b_id),\n",
    "    tools=TOOLBOX_JSON,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "react_code_agent_llama3_70 = ReactCodeAgent(\n",
    "    llm_engine=HfEngine(model=llama3_70b_id),\n",
    "    tools=TOOLBOX_CODE,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "react_code_agent_llama3_8 = ReactCodeAgent(\n",
    "    llm_engine=HfEngine(model=llama3_8b_id),\n",
    "    tools=TOOLBOX_CODE,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "react_code_agent_mixtral_8x7 = ReactCodeAgent(\n",
    "    llm_engine=HfEngine(model=mixtral_id),\n",
    "    tools=TOOLBOX_CODE,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "react_json_agent_mixtral_8x7 = ReactJsonAgent(\n",
    "    llm_engine=HfEngine(model=mixtral_id),\n",
    "    tools=TOOLBOX_JSON,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "react_code_agent_zephyr_large = ReactCodeAgent(\n",
    "    llm_engine=HfEngine(model=zephyr_id),\n",
    "    tools=TOOLBOX_CODE,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "react_json_agent_zephyr_large = ReactJsonAgent(\n",
    "    llm_engine=HfEngine(model=zephyr_id),\n",
    "    tools=TOOLBOX_JSON,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "# react_code_agent_mixtral_large = ReactCodeAgent(\n",
    "#     llm_engine=HfEngine(model=mixtral_large_id),\n",
    "#     tools=TOOLBOX_CODE,\n",
    "#     max_iterations=7,\n",
    "# )\n",
    "\n",
    "# react_json_agent_mixtral_large = ReactJsonAgent(\n",
    "#     llm_engine=HfEngine(model=mixtral_large_id),\n",
    "#     tools=TOOLBOX_JSON,\n",
    "#     max_iterations=7,\n",
    "# )\n",
    "\n",
    "react_code_agent_command_r = ReactCodeAgent(\n",
    "    llm_engine=HfEngine(model=command_r_id),\n",
    "    tools=TOOLBOX_CODE,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "react_json_agent_command_r = ReactJsonAgent(\n",
    "    llm_engine=HfEngine(model=command_r_id),\n",
    "    tools=TOOLBOX_JSON,\n",
    "    max_iterations=7,\n",
    ")\n",
    "\n",
    "\n",
    "agents = {\n",
    "    \"react_code_llama3-70b_17-may\": react_code_agent_llama3_70,\n",
    "    # \"react_json_llama3-70b_17-may\": react_json_agent_llama3_70,\n",
    "    # \"react_code_mixtral_8x7_06-may2\": react_code_agent_mixtral_8x7,\n",
    "    # \"react_json_mixtral_8x7_06-may2\": react_json_agent_mixtral_8x7,\n",
    "    # \"react_code_zephyr_large_06-may2\": react_code_agent_zephyr_large,\n",
    "    # \"react_json_zephyr_large_06-may2\": react_json_agent_zephyr_large,\n",
    "    # \"react_code_command_r_06-may\": react_code_agent_command_r,\n",
    "    # \"react_json_command_r_06-may\": react_json_agent_command_r,\n",
    "    # \"react_json_gpt4_06-may\": react_json_agent_openai,\n",
    "    # \"react_code_gpt4_06-may\": react_code_agent_openai,\n",
    "    # \"langchain_GPT-4_06-may2\": langchain_openai_agent,\n",
    "    # \"langchain-llama3-70B_06-may2\": langchain_agent,\n",
    "    # \"react_code_llama3-8b_24-04\": react_code_agent_llama3_8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def call_transformers_agent(agent, question: str) -> str:\n",
    "    # result = agent.run(question)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    result = await loop.run_in_executor(None, agent.run, question)\n",
    "    return {\n",
    "        \"output\": str(result),\n",
    "        \"intermediate_steps\": agent.logs.copy(),\n",
    "    }\n",
    "\n",
    "\n",
    "async def call_langchain(agent, question: str) -> str:\n",
    "    output = await agent.ainvoke({\"input\": question})\n",
    "    return {\n",
    "        \"output\": output[\"output\"],\n",
    "        \"intermediate_steps\": [el[0].log for el in output[\"intermediate_steps\"]],\n",
    "    }\n",
    "\n",
    "\n",
    "async def call_vanilla_llm(agent, question: str) -> str:\n",
    "    result = agent(question)\n",
    "    return {\n",
    "        \"output\": str(result),\n",
    "        \"intermediate_steps\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "# await call_langchain(langchain_agent, \"Please provide a final answer of '7'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aymeric/Documents/Code/agent_reasoning_benchmark/scripts/run_agents.py:196: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  results = pd.read_json(output_path, lines=True).to_dict(orient=\"records\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected object or value\n",
      "Found no usable records! 🤔 Starting new.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mMimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mmimi_seashells\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m12\u001b[39m\u001b[38;5;7m  \u001b[39m\u001b[38;5;60;03m# 1 dozen = 12\u001b[39;00m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mMimi\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144ms seashells:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mmimi_seashells\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mMimi's seashells: 24\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mkyle_seashells\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mmimi_seashells\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mKyle\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144ms seashells:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mkyle_seashells\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mKyle's seashells: 48\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mleigh_seashells\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mkyle_seashells\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m3\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mleigh_seashells\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m16\u001b[0m\n",
      "  1%|          | 1/90 [00:14<21:33, 14.54s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mFrankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mLet\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144ms break down the information:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144m1. He has six more snakes than he has cats.\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144m2. He has one less parrot than cats.\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144m3. Six of his pets have four legs.\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144m4. He has 2 dogs.\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mLet's break down the information:\n",
      "1. He has six more snakes than he has cats.\n",
      "2. He has one less parrot than cats.\n",
      "3. Six of his pets have four legs.\n",
      "4. He has 2 dogs.\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mcats\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m6\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mNumber of cats:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mcats\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mNumber of cats: 4\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7msnakes\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mcats\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m6\u001b[39m\n",
      "\u001b[38;5;7mparrots\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mcats\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m1\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mNumber of snakes:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7msnakes\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mNumber of parrots:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mparrots\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mNumber of snakes: 10\n",
      "Number of parrots: 3\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mtotal_pets\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mcats\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7msnakes\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mparrots\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mtotal_pets\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m19\u001b[0m\n",
      "  2%|▏         | 2/90 [00:30<22:35, 15.40s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mOlaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7muncle_cars\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7msearch\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mquery\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mOlaf\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144ms auntie gave him 6 toy cars, 1 more than the uncle\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mUncle\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144ms cars:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7muncle_cars\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mUncle's cars: ['Robin had 6 toy cars. Her parents gave her 8 more toy cars for her birthday. How many toy cars did she have then?', 'The most common answers that your students come up with are: Equation 1: G=3-L Equation 2: G-3=L Equation 3: G=L-3 Which equation is correct? G=L-3 A day …', 'Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as …', 'Math-First Grade Addition Word Problems. Tom had 5 toy cars. His dad gave him 3 more toy cars. How many toy cars did Tom have in all? Click the card to flip 👆. Tom would …', 'Jack bought some erasers, pencils and rulers. 1/6 of them were erasers. The number of pencils he bought was 5 more 1/2 the total number of all items and the remaining were …', 'Sabrina has 4 toy cars. For her birthday she gets 2 more. 4 + 2 4 + 3 4. −. 2. How many toy cars does Sabrina have? 4. 5. 6. Read the word problem and click on the matching …', 'Lucy had 3m more apples than oranges. She gave away 9m apples and was left with 2/5 as many apples as oranges. How many oranges did Lucy have, in terms of m? Show Solution', 'Example 6: The sum of Aaliyah and Harald’s ages is 28. Four years from now, Aaliyah will be three times as old as Harald. Find their present ages. Neither Aaliyah nor Harald’s …', 'Do you need help with algebraic word problems? Wyzant offers a platform where you can ask experts and get answers with the equation and solution. Check out …', 'Tyrese wanted to run 8 miles, but he only ran 1/4 of the 8 miles; so, how many miles did he run? A 1/4 + 8 B 1/2 divide 8 C 1/4 divide 8 D 1/4 x 8 Chocolates 5764']\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7muncle_cars\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m5\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mUncle\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144ms cars:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7muncle_cars\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mUncle's cars: 5\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mgrandpa_cars\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m5\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mGrandpa\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144ms cars:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mgrandpa_cars\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mGrandpa's cars: 10\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mmum_cars\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m10\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m5\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mMum\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144ms cars:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mmum_cars\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mMum's cars: 15\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mtotal_cars\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m150\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m5\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m10\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m10\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m15\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m6\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mtotal_cars\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m196\u001b[0m\n",
      "  3%|▎         | 3/90 [00:59<31:24, 21.66s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mEmma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7minitial_balance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m100\u001b[39m\n",
      "\u001b[38;5;7mdaily_expense\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m8\u001b[39m\n",
      "\u001b[38;5;7mnum_days\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m7\u001b[39m\n",
      "\u001b[38;5;7mtotal_expense\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mdaily_expense\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mnum_days\u001b[39m\n",
      "\u001b[38;5;7mnew_balance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7minitial_balance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtotal_expense\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mNew balance:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mnew_balance\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mNew balance: 44\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mnum_five_dollar_bills\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mnew_balance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m5\u001b[39m\n",
      "\u001b[38;5;7mvalue_of_bills\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mnum_five_dollar_bills\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m5\u001b[39m\n",
      "\u001b[38;5;7mremaining_amount\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mnew_balance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mvalue_of_bills\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mRemaining amount:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mremaining_amount\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mRemaining amount: 4\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;139m4\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m4\u001b[0m\n",
      "  4%|▍         | 4/90 [01:15<27:34, 19.23s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mEzekiel hikes as a hobby. This past summer, he did a challenging three-day hike across 50 kilometers of wilderness. The first day, he covered 10 kilometers of steep mountainside. The second day was flatter and he was able to cover half the full hike distance. How many kilometers did he have to hike on the third day to finish the hike?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mday1_distance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m10\u001b[39m\n",
      "\u001b[38;5;7mday2_distance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m50\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\u001b[38;5;7m  \u001b[39m\u001b[38;5;60;03m# half of the full hike distance\u001b[39;00m\n",
      "\u001b[38;5;7mtotal_distance_covered\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mday1_distance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mday2_distance\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mTotal distance covered in first two days:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtotal_distance_covered\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mTotal distance covered in first two days: 35.0\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mtotal_hike_distance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m50\u001b[39m\n",
      "\u001b[38;5;7mremaining_distance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtotal_hike_distance\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtotal_distance_covered\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mRemaining distance to hike on the third day:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mremaining_distance\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mRemaining distance to hike on the third day: 15.0\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144m15 kilometers\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m15 kilometers\u001b[0m\n",
      "  6%|▌         | 5/90 [01:59<39:59, 28.22s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mJames decides to build a tin house by collecting 500 tins in a week. On the first day, he collects 50 tins. On the second day, he manages to collect 3 times that number. On the third day, he collects 50 tins fewer than the number he collected on the second day. If he collects an equal number of tins on the remaining days of the week, what's the number of tins he collected each day for the rest of the week?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mday1\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m50\u001b[39m\n",
      "\u001b[38;5;7mday2\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m3\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mday1\u001b[39m\n",
      "\u001b[38;5;7mday3\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mday2\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m50\u001b[39m\n",
      "\u001b[38;5;7mtotal_collected\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mday1\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mday2\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mday3\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mTotal collected in 3 days:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtotal_collected\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mTotal collected in 3 days: 300\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mremaining_tins\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m500\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m300\u001b[39m\n",
      "\u001b[38;5;7mdaily_target\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mremaining_tins\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m4\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mRemaining tins needed:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mremaining_tins\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mDaily target for the remaining days:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mdaily_target\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mRemaining tins needed: 200\n",
      "Daily target for the remaining days: 50.0\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;139m50.0\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m50.0\u001b[0m\n",
      "  7%|▋         | 6/90 [02:57<53:43, 38.37s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mDon throws 3 darts.  One is a bullseye worth 50 points.  One completely missed the target, so received no points. The third was worth half the points of the bullseye.  What is the final score from these 3 throws?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mbullseye_score\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m50\u001b[39m\n",
      "\u001b[38;5;7mthird_dart_score\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mbullseye_score\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\n",
      "\u001b[38;5;7msecond_dart_score\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m0\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mBullseye score:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mbullseye_score\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mThird dart score:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mthird_dart_score\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mSecond dart score:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7msecond_dart_score\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mBullseye score: 50\n",
      "Third dart score: 25.0\n",
      "Second dart score: 0\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mfinal_score\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mbullseye_score\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mthird_dart_score\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7msecond_dart_score\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mfinal_score\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m75.0\u001b[0m\n",
      "  8%|▊         | 7/90 [03:27<49:30, 35.79s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mTreQuan is throwing rocks in the river and he notices that the bigger the rock, the wider the splash. Pebbles make a splash that is a 1/4 meter wide. Rocks make a splash that is 1/2 a meter wide, and boulders create a splash that is 2 meters wide. If he tosses 6 pebbles, 3 rocks, and 2 boulders, what is the total width of the splashes he makes?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mpebble_splash\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m1\u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;139m4\u001b[39m\n",
      "\u001b[38;5;7mrock_splash\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m1\u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;139m2\u001b[39m\n",
      "\u001b[38;5;7mboulder_splash\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mPebble splash:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mpebble_splash\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mRock splash:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mrock_splash\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mBoulder splash:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mboulder_splash\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mPebble splash: 0.25\n",
      "Rock splash: 0.5\n",
      "Boulder splash: 2\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mtotal_splash\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m6\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m0.25\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m3\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m0.5\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mTotal splash:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtotal_splash\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mTotal splash: 7.0\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144m7.0\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m7.0\u001b[0m\n",
      "  9%|▉         | 8/90 [03:52<43:59, 32.19s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mPauly is making omelets for his family. There are three dozen eggs, and he plans to use them all. Each omelet requires 4 eggs. Including himself, there are 3 people. How many omelets does each person get?\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mtotal_eggs\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m3\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m*\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m12\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mTotal eggs:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtotal_eggs\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mTotal eggs: 36\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mtotal_omelets\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m36\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m4\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mTotal omelets:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtotal_omelets\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mTotal omelets: 9\n",
      "\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7momelets_per_person\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m9\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;109;01m/\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m3\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7momelets_per_person\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m3\u001b[0m\n",
      " 10%|█         | 9/90 [04:20<41:40, 30.87s/it]\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mThomas made 4 stacks of wooden blocks. The first stack was 7 blocks tall. The second stack was 3 blocks taller than the first. The third stack was 6 blocks shorter than the second stack, and the fourth stack was 10 blocks taller than the third stack. If the fifth stack has twice as many blocks as the second stack, how many blocks did Thomas use in all?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from scripts.run_agents import run_full_tests\n",
    "\n",
    "results = await run_full_tests(\n",
    "    eval_ds,\n",
    "    agents,\n",
    "    output_folder=OUTPUT_DIR,\n",
    "    agent_call_function=call_transformers_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "answer_file_path = f\"{OUTPUT_DIR}/answers.jsonl\"\n",
    "\n",
    "res = []\n",
    "for f in glob.glob(f\"{OUTPUT_DIR}/*.jsonl\"):\n",
    "    print(f)\n",
    "    if \"answers\" not in f and \"evaluation\" not in f:\n",
    "        res.append(pd.read_json(f, lines=True))\n",
    "result_df = pd.concat(res)\n",
    "result_df = result_df.drop(columns=[\"start_time\", \"end_time\"])\n",
    "result_df.to_json(answer_file_path, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact match for GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluation import score_any_match\n",
    "\n",
    "results_math = result_df.loc[result_df[\"task\"] == \"gsm8k\"].copy()\n",
    "results_math[\"exact_match\"] = -1\n",
    "results_math[\"exact_match\"] = results_math.apply(\n",
    "    lambda row: score_any_match(row[\"prediction\"], float(row[\"true_answer\"])), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_name                       task \n",
       "langchain-llama3-70B_06-may      gsm8k    0.825\n",
       "langchain-llama3-70B_06-may2     gsm8k    0.900\n",
       "langchain-llama3-70B_29-04       gsm8k    0.750\n",
       "langchain-mixtral-8x7b_29-04     gsm8k    0.475\n",
       "langchain_GPT-4_06-may           gsm8k    0.875\n",
       "langchain_GPT-4_06-may2          gsm8k    0.850\n",
       "langchain_GPT-4_29-04            gsm8k    0.825\n",
       "react_code_command_r_06-may      gsm8k    0.725\n",
       "react_code_command_r_29-04       gsm8k    0.775\n",
       "react_code_gpt4_06-may           gsm8k    0.975\n",
       "react_code_gpt4_29-04            gsm8k    0.975\n",
       "react_code_llama3-70b_06-may     gsm8k    0.950\n",
       "react_code_llama3-70b_06-may2    gsm8k    0.975\n",
       "react_code_llama3-70b_15-may     gsm8k    0.900\n",
       "react_code_llama3-70b_29-04      gsm8k    0.925\n",
       "react_code_mixtral_8x7_06-may    gsm8k    0.675\n",
       "react_code_mixtral_8x7_06-may2   gsm8k    0.750\n",
       "react_code_mixtral_8x7_29-04     gsm8k    0.625\n",
       "react_code_zephyr_large_06-may   gsm8k    0.725\n",
       "react_code_zephyr_large_06-may2  gsm8k    0.675\n",
       "react_code_zephyr_large_29-04    gsm8k    0.675\n",
       "react_json_command_r_06-may      gsm8k    0.650\n",
       "react_json_command_r_29-04       gsm8k    0.525\n",
       "react_json_gpt4_06-may           gsm8k    0.950\n",
       "react_json_gpt4_29-04            gsm8k    0.950\n",
       "react_json_llama3-70b_06-may     gsm8k    0.950\n",
       "react_json_llama3-70b_06-may2    gsm8k    0.925\n",
       "react_json_llama3-70b_15-may     gsm8k    0.925\n",
       "react_json_llama3-70b_29-04      gsm8k    0.950\n",
       "react_json_mixtral_8x7_06-may    gsm8k    0.625\n",
       "react_json_mixtral_8x7_06-may2   gsm8k    0.775\n",
       "react_json_mixtral_8x7_29-04     gsm8k    0.600\n",
       "react_json_zephyr_large_06-may   gsm8k    0.750\n",
       "react_json_zephyr_large_06-may2  gsm8k    0.700\n",
       "react_json_zephyr_large_29-04    gsm8k    0.600\n",
       "Name: exact_match, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_math.groupby([\"agent_name\", \"task\"])[\"exact_match\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_name                       task \n",
       "langchain-llama3-70B_06-may      gsm8k    40\n",
       "react_json_llama3-70b_06-may2    gsm8k    40\n",
       "react_code_zephyr_large_29-04    gsm8k    40\n",
       "react_json_command_r_06-may      gsm8k    40\n",
       "react_json_command_r_29-04       gsm8k    40\n",
       "react_json_gpt4_06-may           gsm8k    40\n",
       "react_json_gpt4_29-04            gsm8k    40\n",
       "react_json_llama3-70b_06-may     gsm8k    40\n",
       "react_json_llama3-70b_15-may     gsm8k    40\n",
       "react_code_zephyr_large_06-may   gsm8k    40\n",
       "react_json_llama3-70b_29-04      gsm8k    40\n",
       "react_json_mixtral_8x7_06-may    gsm8k    40\n",
       "react_json_mixtral_8x7_06-may2   gsm8k    40\n",
       "react_json_mixtral_8x7_29-04     gsm8k    40\n",
       "react_json_zephyr_large_06-may   gsm8k    40\n",
       "react_json_zephyr_large_06-may2  gsm8k    40\n",
       "react_code_zephyr_large_06-may2  gsm8k    40\n",
       "react_code_mixtral_8x7_29-04     gsm8k    40\n",
       "langchain-llama3-70B_06-may2     gsm8k    40\n",
       "react_code_command_r_29-04       gsm8k    40\n",
       "langchain-llama3-70B_29-04       gsm8k    40\n",
       "langchain-mixtral-8x7b_29-04     gsm8k    40\n",
       "langchain_GPT-4_06-may           gsm8k    40\n",
       "langchain_GPT-4_06-may2          gsm8k    40\n",
       "langchain_GPT-4_29-04            gsm8k    40\n",
       "react_code_command_r_06-may      gsm8k    40\n",
       "react_code_gpt4_06-may           gsm8k    40\n",
       "react_code_mixtral_8x7_06-may2   gsm8k    40\n",
       "react_code_gpt4_29-04            gsm8k    40\n",
       "react_code_llama3-70b_06-may     gsm8k    40\n",
       "react_code_llama3-70b_06-may2    gsm8k    40\n",
       "react_code_llama3-70b_15-may     gsm8k    40\n",
       "react_code_llama3-70b_29-04      gsm8k    40\n",
       "react_code_mixtral_8x7_06-may    gsm8k    40\n",
       "react_json_zephyr_large_29-04    gsm8k    40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_math[[\"agent_name\", \"task\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM judge for others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_nonmath = result_df.loc[result_df[\"task\"] != \"gsm8k\"].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "eval_model_name = \"GPT4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations = pd.read_json('output_reasoning/evaluation.jsonl', lines=True)\n",
    "# print(len(evaluations))\n",
    "# evaluations['agent_name'].unique()\n",
    "\n",
    "# evaluations = evaluations.loc[~evaluations[\"agent_name\"].str.contains(\"8b\")]\n",
    "# print(len(evaluations))\n",
    "\n",
    "# evaluations.to_json('output_reasoning/evaluation.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:38<00:00,  2.60it/s]\n",
      "Exception in thread Thread-7 (write_line):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self.run()\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/aymeric/Documents/Code/agent_reasoning_benchmark/scripts/evaluation.py\", line 130, in write_line\n",
      "    json.dump(annotated_example, output_file)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py\", line 180, in dump\n",
      "    fp.write(chunk)\n",
      "ValueError: write to closed file\n"
     ]
    }
   ],
   "source": [
    "from scripts.evaluation import evaluate_answers\n",
    "from scripts.prompts import EVALUATION_PROMPT_TEMPLATE\n",
    "\n",
    "output_file_path = f\"{OUTPUT_DIR}/evaluation.jsonl\"\n",
    "\n",
    "run_evaluation = True\n",
    "if run_evaluation:\n",
    "    evaluated_answers_nonmath = await evaluate_answers(\n",
    "        answers_nonmath,\n",
    "        eval_chat_model,\n",
    "        \"GPT4\",\n",
    "        EVALUATION_PROMPT_TEMPLATE,\n",
    "        output_file_path=output_file_path,\n",
    "    )\n",
    "    print(\"Evaluation is complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(evaluated_answers_nonmath)\n",
    "# df.loc[(df[\"agent_name\"] == \"react_code_llama3-8b_24-04\")].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_name\n",
       "langchain-llama3-70B_06-may        0.437500\n",
       "langchain-llama3-70B_06-may2       0.300000\n",
       "langchain-llama3-70B_29-04         0.262500\n",
       "langchain-mixtral-8x7b_29-04       0.175000\n",
       "langchain_GPT-4_06-may             0.453125\n",
       "langchain_GPT-4_06-may2            0.537500\n",
       "langchain_GPT-4_29-04              0.412500\n",
       "react_code_command_r_06-may        0.000000\n",
       "react_code_command_r_29-04         0.225000\n",
       "react_code_gpt4_06-may             0.425000\n",
       "react_code_gpt4_23-04              0.512500\n",
       "react_code_gpt4_25-04              0.600000\n",
       "react_code_gpt4_26-04              0.387500\n",
       "react_code_gpt4_29-04              0.350000\n",
       "react_code_llama3-70b_06-may       0.412500\n",
       "react_code_llama3-70b_06-may2      0.400000\n",
       "react_code_llama3-70b_15-may       0.412500\n",
       "react_code_llama3-70b_23-04        0.323529\n",
       "react_code_llama3-70b_24-04        0.325000\n",
       "react_code_llama3-70b_25-04        0.300000\n",
       "react_code_llama3-70b_26-04        0.362500\n",
       "react_code_llama3-70b_29-04        0.262500\n",
       "react_code_llama3-8b_24-04         0.037500\n",
       "react_code_mixtral_8x7_06-may      0.200000\n",
       "react_code_mixtral_8x7_06-may2     0.062500\n",
       "react_code_mixtral_8x7_23-04       0.000000\n",
       "react_code_mixtral_8x7_29-04       0.075000\n",
       "react_code_zephyr_large_06-may     0.300000\n",
       "react_code_zephyr_large_06-may2    0.075000\n",
       "react_code_zephyr_large_29-04      0.175000\n",
       "react_json_command_r_06-may        0.137500\n",
       "react_json_command_r_29-04         0.287500\n",
       "react_json_gpt4_06-may             0.462500\n",
       "react_json_gpt4_26-04              0.425000\n",
       "react_json_gpt4_29-04              0.375000\n",
       "react_json_llama3-70b_06-may       0.450000\n",
       "react_json_llama3-70b_06-may2      0.350000\n",
       "react_json_llama3-70b_15-may       0.450000\n",
       "react_json_llama3-70b_23-04        0.409091\n",
       "react_json_llama3-70b_26-04        0.262500\n",
       "react_json_llama3-70b_29-04        0.375000\n",
       "react_json_mixtral_8x7_06-may      0.212500\n",
       "react_json_mixtral_8x7_06-may2     0.000000\n",
       "react_json_mixtral_8x7_24-04       0.150000\n",
       "react_json_mixtral_8x7_29-04       0.166667\n",
       "react_json_zephyr_large_06-may     0.075000\n",
       "react_json_zephyr_large_29-04      0.212500\n",
       "Name: eval_score_GPT4, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_nonmath = pd.DataFrame.from_dict(evaluated_answers_nonmath)\n",
    "\n",
    "\n",
    "def interpret_score(eval_score):\n",
    "    try:\n",
    "        return (float(eval_score) - 1) / 4\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "results_nonmath[\"eval_score_GPT4\"] = results_nonmath[\"eval_score_GPT4\"].apply(\n",
    "    interpret_score\n",
    ")\n",
    "results_nonmath.loc[results_nonmath[\"task\"] == \"GAIA\"].groupby(\"agent_name\")[\n",
    "    \"eval_score_GPT4\"\n",
    "].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_name                  \n",
       "langchain-llama3-70B_06-may     90\n",
       "langchain-llama3-70B_06-may2    90\n",
       "langchain-llama3-70B_29-04      90\n",
       "langchain-mixtral-8x7b_29-04    90\n",
       "langchain_GPT-4_06-may2         90\n",
       "langchain_GPT-4_29-04           90\n",
       "langchain_GPT-4_06-may          86\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[result_df[\"agent_name\"].str.contains(\"langchain\")][\n",
    "    [\"agent_name\"]\n",
    "].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_name                      task    \n",
       "langchain-llama3-70B_06-may     GAIA           36.875\n",
       "                                HotpotQA    60.416667\n",
       "                                gsm8k           86.25\n",
       "langchain-llama3-70B_29-04      GAIA            26.25\n",
       "                                HotpotQA    45.833333\n",
       "                                gsm8k            75.0\n",
       "langchain-mixtral-8x7b_29-04    GAIA             17.5\n",
       "                                HotpotQA         60.0\n",
       "                                gsm8k            47.5\n",
       "langchain_GPT-4_06-may          GAIA             50.0\n",
       "                                HotpotQA    74.166667\n",
       "                                gsm8k           86.25\n",
       "langchain_GPT-4_29-04           GAIA            41.25\n",
       "                                HotpotQA    71.666667\n",
       "                                gsm8k            82.5\n",
       "react_code_command_r_06-may     GAIA              0.0\n",
       "                                HotpotQA    63.333333\n",
       "                                gsm8k            72.5\n",
       "react_code_command_r_29-04      GAIA             22.5\n",
       "                                HotpotQA         67.5\n",
       "                                gsm8k            77.5\n",
       "react_code_gpt4_06-may          GAIA             42.5\n",
       "                                HotpotQA    69.166667\n",
       "                                gsm8k            97.5\n",
       "react_code_gpt4_23-04           GAIA            51.25\n",
       "                                HotpotQA    60.833333\n",
       "react_code_gpt4_25-04           GAIA             60.0\n",
       "                                HotpotQA    73.333333\n",
       "react_code_gpt4_26-04           GAIA            38.75\n",
       "                                HotpotQA    69.166667\n",
       "react_code_gpt4_29-04           GAIA             35.0\n",
       "                                HotpotQA    68.333333\n",
       "                                gsm8k            97.5\n",
       "react_code_llama3-70b_06-may    GAIA           40.625\n",
       "                                HotpotQA    73.333333\n",
       "                                gsm8k           96.25\n",
       "react_code_llama3-70b_15-may    GAIA            41.25\n",
       "                                HotpotQA    68.333333\n",
       "                                gsm8k            90.0\n",
       "react_code_llama3-70b_23-04     GAIA        32.352941\n",
       "                                HotpotQA    65.833333\n",
       "react_code_llama3-70b_24-04     GAIA             32.5\n",
       "                                HotpotQA    60.833333\n",
       "react_code_llama3-70b_25-04     GAIA             30.0\n",
       "                                HotpotQA    58.333333\n",
       "react_code_llama3-70b_26-04     GAIA            36.25\n",
       "                                HotpotQA    61.666667\n",
       "react_code_llama3-70b_29-04     GAIA            26.25\n",
       "                                HotpotQA    63.333333\n",
       "                                gsm8k            92.5\n",
       "react_code_llama3-8b_24-04      GAIA             3.75\n",
       "                                HotpotQA    24.166667\n",
       "react_code_mixtral_8x7_06-may   GAIA           13.125\n",
       "                                HotpotQA    58.333333\n",
       "                                gsm8k           71.25\n",
       "react_code_mixtral_8x7_23-04    GAIA              0.0\n",
       "                                HotpotQA         42.5\n",
       "react_code_mixtral_8x7_29-04    GAIA              7.5\n",
       "                                HotpotQA    51.666667\n",
       "                                gsm8k            62.5\n",
       "react_code_zephyr_large_06-may  GAIA            18.75\n",
       "                                HotpotQA         45.0\n",
       "                                gsm8k            70.0\n",
       "react_code_zephyr_large_29-04   GAIA             17.5\n",
       "                                HotpotQA    44.166667\n",
       "                                gsm8k            67.5\n",
       "react_json_command_r_06-may     GAIA            13.75\n",
       "                                HotpotQA    70.833333\n",
       "                                gsm8k            65.0\n",
       "react_json_command_r_29-04      GAIA            28.75\n",
       "                                HotpotQA         77.5\n",
       "                                gsm8k            52.5\n",
       "react_json_gpt4_06-may          GAIA            46.25\n",
       "                                HotpotQA    70.833333\n",
       "                                gsm8k            95.0\n",
       "react_json_gpt4_26-04           GAIA             42.5\n",
       "                                HotpotQA    68.333333\n",
       "react_json_gpt4_29-04           GAIA             37.5\n",
       "                                HotpotQA    78.333333\n",
       "                                gsm8k            95.0\n",
       "react_json_llama3-70b_06-may    GAIA             40.0\n",
       "                                HotpotQA    72.916667\n",
       "                                gsm8k           93.75\n",
       "react_json_llama3-70b_15-may    GAIA             45.0\n",
       "                                HotpotQA    74.166667\n",
       "                                gsm8k            92.5\n",
       "react_json_llama3-70b_23-04     GAIA        40.909091\n",
       "                                HotpotQA    71.666667\n",
       "react_json_llama3-70b_26-04     GAIA            26.25\n",
       "                                HotpotQA    65.833333\n",
       "react_json_llama3-70b_29-04     GAIA             37.5\n",
       "                                HotpotQA    74.166667\n",
       "                                gsm8k            95.0\n",
       "react_json_mixtral_8x7_06-may   GAIA        17.708333\n",
       "                                HotpotQA        63.75\n",
       "                                gsm8k            70.0\n",
       "react_json_mixtral_8x7_24-04    GAIA             15.0\n",
       "                                HotpotQA        61.25\n",
       "react_json_mixtral_8x7_29-04    GAIA        16.666667\n",
       "                                HotpotQA    65.833333\n",
       "                                gsm8k            60.0\n",
       "react_json_zephyr_large_06-may  GAIA              7.5\n",
       "                                HotpotQA    55.833333\n",
       "                                gsm8k            72.5\n",
       "react_json_zephyr_large_29-04   GAIA            21.25\n",
       "                                HotpotQA         47.5\n",
       "                                gsm8k            60.0\n",
       "Name: aggregate_score, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df = pd.concat([results_math, results_nonmath])\n",
    "result_df[\"aggregate_score\"] = (\n",
    "    result_df[\"exact_match\"].fillna(0) + result_df[\"eval_score_GPT4\"].fillna(0)\n",
    ") * 100\n",
    "\n",
    "result_df[\"agent_name\"] = result_df[\"agent_name\"].str.replace(\"06-may2\", \"06-may\")\n",
    "display(result_df.groupby([\"agent_name\", \"task\"])[\"aggregate_score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_output': 'Thought: I will use python code to calculate the total number of pets.\\nCode:\\n```\\nsnakes = cats + 6\\nparrots = cats - 1\\ntotal_pets = cats + snakes + parrots + dogs\\nprint(\"Total pets:\", total_pets)\\n```',\n",
       " 'rationale': 'Thought: I will use python code to calculate the total number of pets.\\n',\n",
       " 'tool_call': {'tool_name': 'code interpreter',\n",
       "  'tool_arguments': 'snakes = cats + 6\\nparrots = cats - 1\\ntotal_pets = cats + snakes + parrots + dogs\\nprint(\"Total pets:\", total_pets)'},\n",
       " 'observation': '\\nTotal pets: 7',\n",
       " 'agent_memory': [{'role': 'system',\n",
       "   'content': 'Solve the following task as best you can. You have access to the following tools:\\n\\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of \\'Thought:\\', \\'Code:\\', and \\'Observation:\\' sequences.\\n\\nAt each step, in the \\'Thought:\\' sequence, you should first explain which tool you will use and for what reason, then in the \\'Code:\\' sequence, you shold write the code in simple Python. The code sequence must end with \\'/End code\\' sequence.\\nDuring each intermediate step, you can use \\'print()\\' to save whatever important information you will then need.\\nIt will then be available in the \\'Observation:\\' field, for using this information as input for the next step.\\n\\nIn the end you have to return a final answer using the `final_answer` tool.\\nOnly when you use function final_answer() will your final answer be returned.\\n\\nYou have access to the following tools:\\n\\n- search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\n    Takes inputs: {\\'query\\': {\\'description\\': \\'your search query\\', \\'type\\': \\'text\\'}}\\n\\n- final_answer: Provides a final answer to the given problem\\n    Takes inputs: {\\'answer\\': {\\'type\\': \\'text\\', \\'description\\': \\'The final answer to the problem\\'}}\\n\\nYou will be provided a task:\\nTask: the task you are given.\\n\\nThen you should answer as follows:\\n\\nThought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.\\nCode:\\n```py\\ntranslated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\\nanswer = image_qa(image=image, question=translated_question)\\nfinal_answer(f\"The answer is {answer}\")\\n```<end_code>\\n\\nThe \\'Action input:\\' should be the input for the tool you are using.\\nMake sure to have the $ACTION_INPUT as a dictionnary, with the right keys for the tool you are using, and do not put variable names as input if you know the right values.\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\nCode:\\n```py\\nanswer = document_qa(document=document, question=\"What is the oldest person?\")\\nprint(answer)\\n```<end_code>\\nObservation: \"The oldest person in the document is John Doe.\"\\n\\nThought: I will now generate an image showcasing the oldest person.\\n\\nCode:\\n```py\\nimage = image_generator(\"An image of John Doe, an old man.\")\\nfinal_answer(image)\\n```<end_code>\\n\\nExample:::\\nTask: \"Generate an image using the text given in the variable `caption`.\"\\n\\nThought: I will use the following tool: `image_generator` to generate an image.\\nCode:\\n```py\\nimage = image_generator(prompt=caption)\\nfinal_answer(image)\\n```<end_code>\\n\\nExample:::\\nTask: \"Summarize the text given in the variable `text` and read it out loud.\"\\n\\nThought: I will use the following tools: `summarizer` to create a summary of the input text, then `text_reader` to read it out loud.\\n\\nCode:\\n```py\\nsummarized_text = summarizer(text)\\nprint(f\"Summary: {summarized_text}\")\\n```<end_code>\\n\\nObservation: \"Summary: \\'The text is about the history of the Roman Empire.\\'\"\\n\\nThought: I will now read the summarized text out loud.\\n\\nCode:\\n```py\\naudio_summary = text_reader(summarized_text)\\nfinal_answer(audio_summary)\\n```<end_code>\\n\\nExample:::\\nTask: \"Answer the question in the variable `question` about the text in the variable `text`. Use the answer to generate an image.\"\\n\\nThought: I will use the following tools: `text_qa` to create the answer, then `image_generator` to generate an image according to the answer.\\n\\nBe sure to provide an \\'Code:\\' token, else the system will be stuck in a loop.\\n\\nCode:\\n```py\\nanswer = text_qa(text=text, question=question)\\nimage = image_generator(answer)\\nfinal_answer(image)\\n```<end_code>\\n\\nExample:::\\nTask: \"What is the result of the following operation: 5 + 3 + 1298987654.6789098765?\"\\n\\nThought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\\n\\nCode:\\n```py\\nresult = 5 + 3 + 1298987654.6789098765\\nfinal_answer(result)\\n```<end_code>\\n\\nExample:::\\nTask: \"Which city has the highest population , Guangzhou or Shanghai?\"\\n\\nThought: I will use the tool `search` to get the population of both cities.\\nCode:\\n```py\\npopulation_guangzhou = search(\"Guangzhou population\")\\nprint(\"Population Guangzhou:\", population_guangzhou)\\npopulation_shanghai = search(\"Shanghai population\")\\nprint(\"Population Shanghai:\", population_shanghai)\\n```<end_code>\\nObservation:\\nPopulation Guangzhou: [\\'Guangzhou has a population of 15 million inhabitants as of 2021.\\']\\nPopulation Shanghai: \\'24 million\\'\\n\\nThought: I know that Shanghai has the highest population.\\nCode:\\n```py\\nfinal_answer(\"Shanghai\")\\n```<end_code>\\n\\nExample:::\\nTask: \"What is the current age of the pope, raised to the power 0.36?\"\\n\\nThought: I will use the tool `search` to get the age of the pope, then raise it to the power 0.36.\\nCode:\\n```py\\npope_age = search(query=\"current pope age\")\\nprint(\"Pope age:\", pope_age)\\n```<end_code>\\n\\nObservation:\\nPope age: \"The pope Francis is currently 85 years old.\"\\n\\nThought: I know that the pope is 85 years old. Let\\'s compute the result using python code.\\nCode:\\n```py\\npope_current_age = 85 ** 0.36\\nfinal_answer(pope_current_age)\\n```<end_code>\\n\\n\\nAbove example were using tools that might not exist for you. You only have acces to those tools:\\n\\'search\\', \\'final_answer\\'\\nYou also can perform computations in the python code you generate.\\n\\nAlways provide a \\'Thought:\\' and an \\'Code:\\n```py\\' sequence ending with \\'```<end_code>\\' sequence. You MUST provide at least the \\'Code:\\' sequence to move forward.\\n\\nRemember to not perform too many operations in a single code block! You should split the task into intermediate code blocks. Then use one single print() at the end of each step to save the intermediate result. Then use final_answer() to return the final result.\\n\\nDO NOT pass the arguments as a dict as in \\'answer = ask_search_agent({\\'query\\': \"What is the place where James Bond lives?\"})\\', but use the arguments directly as in \\'answer = ask_search_agent(query=\"What is the place where James Bond lives?\")\\'.\\n\\nNow Begin! \\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"Task: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Thought: I will use the tool `search` to understand the problem and then use python code to solve it step by step.\\nCode:\\n```\\nprint(\"Let\\'s break down the problem step by step.\")\\n```\\n<end_code>\\n\\n\\n'},\n",
       "  {'role': 'tool-response',\n",
       "   'content': \"Observation: \\nLet's break down the problem step by step.\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Thought: I will use python code to set up the variables and equations based on the problem statement.\\nCode:\\n```\\ncats = 0  # unknown number of cats\\nsnakes = cats + 6  # 6 more snakes than cats\\nparrots = cats - 1  # 1 less parrot than cats\\ndogs = 2  # 2 dogs\\nfour_legged_pets = 6  # 6 pets with four legs (including dogs)\\nprint(\"Variables set up.\")\\n```\\n\\n'},\n",
       "  {'role': 'tool-response', 'content': 'Observation: \\nVariables set up.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Thought: I will use python code to set up an equation based on the fact that the total number of four-legged pets is 6, and dogs are included in this count.\\nCode:\\n```\\nfour_legged_pets = cats + dogs\\nprint(\"Equation set up.\")\\n```\\n\\n'},\n",
       "  {'role': 'tool-response', 'content': 'Observation: \\nEquation set up.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Thought: I will use python code to solve for the number of cats.\\nCode:\\n```\\ncats = four_legged_pets - dogs\\nprint(\"Cats:\", cats)\\n```\\n'},\n",
       "  {'role': 'tool-response', 'content': 'Observation: \\nCats: 0'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Thought: I will use python code to calculate the total number of pets.\\nCode:\\n```\\nsnakes = cats + 6\\nparrots = cats - 1\\ntotal_pets = cats + snakes + parrots + dogs\\nprint(\"Total pets:\", total_pets)\\n```\\n'},\n",
       "  {'role': 'tool-response', 'content': 'Observation: \\nTotal pets: 7'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[\"intermediate_steps\"].values[1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.loc[result_df[\"intermediate_steps\"].apply(lambda x: \"error\" in str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "<b>Task</b>=gsm8k<br><b>LLM Engine</b>=%{x}<br><b>Performance</b>=%{y}<extra></extra>",
         "legendgroup": "gsm8k",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "gsm8k",
         "offsetgroup": "gsm8k",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.0f}",
         "type": "bar",
         "x": [
          "Mixtral-8x7B JSON",
          "Mixtral-8x7B Code",
          "Llama3-70B-Instruct JSON 2",
          "Llama3-70B-Instruct JSON",
          "Llama3-70B-Instruct Code 2",
          "Llama3-70B-Instruct Code",
          "GPT-4-Turbo JSON",
          "GPT-4-Turbo Code",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
         ],
         "xaxis": "x",
         "y": [
          70,
          71.25,
          92.5,
          93.75,
          90,
          96.25,
          95,
          97.5,
          86.25,
          75,
          47.5,
          86.25,
          82.5,
          72.5,
          77.5,
          97.5,
          92.5,
          62.5,
          70,
          67.5,
          65,
          52.5,
          95,
          95,
          60,
          72.5,
          60
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "<b>Task</b>=HotpotQA<br><b>LLM Engine</b>=%{x}<br><b>Performance</b>=%{y}<extra></extra>",
         "legendgroup": "HotpotQA",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "HotpotQA",
         "offsetgroup": "HotpotQA",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.0f}",
         "type": "bar",
         "x": [
          "Mixtral-8x7B JSON",
          "Mixtral-8x7B Code",
          "Llama3-70B-Instruct JSON 2",
          "Llama3-70B-Instruct JSON",
          "Llama3-70B-Instruct Code 2",
          "Llama3-70B-Instruct Code",
          "GPT-4-Turbo JSON",
          "GPT-4-Turbo Code",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
         ],
         "xaxis": "x",
         "y": [
          63.75,
          58.333333333333336,
          74.16666666666667,
          72.91666666666667,
          68.33333333333333,
          73.33333333333333,
          70.83333333333333,
          69.16666666666667,
          60.416666666666664,
          45.833333333333336,
          60,
          74.16666666666667,
          71.66666666666667,
          63.333333333333336,
          67.5,
          60.833333333333336,
          73.33333333333333,
          69.16666666666667,
          68.33333333333333,
          65.83333333333333,
          60.833333333333336,
          58.333333333333336,
          61.666666666666664,
          63.333333333333336,
          24.166666666666668,
          42.5,
          51.666666666666664,
          45,
          44.166666666666664,
          70.83333333333333,
          77.5,
          68.33333333333333,
          78.33333333333333,
          71.66666666666667,
          65.83333333333333,
          74.16666666666667,
          61.25,
          65.83333333333333,
          55.833333333333336,
          47.5
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "<b>Task</b>=GAIA<br><b>LLM Engine</b>=%{x}<br><b>Performance</b>=%{y}<extra></extra>",
         "legendgroup": "GAIA",
         "marker": {
          "color": "#00cc96",
          "pattern": {
           "shape": ""
          }
         },
         "name": "GAIA",
         "offsetgroup": "GAIA",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.0f}",
         "type": "bar",
         "x": [
          "Mixtral-8x7B JSON",
          "Mixtral-8x7B Code",
          "Llama3-70B-Instruct JSON 2",
          "Llama3-70B-Instruct JSON",
          "Llama3-70B-Instruct Code 2",
          "Llama3-70B-Instruct Code",
          "GPT-4-Turbo JSON",
          "GPT-4-Turbo Code",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
         ],
         "xaxis": "x",
         "y": [
          17.708333333333332,
          13.125,
          45,
          40,
          41.25,
          40.625,
          46.25,
          42.5,
          36.875,
          26.25,
          17.5,
          50,
          41.25,
          0,
          22.5,
          51.25,
          60,
          38.75,
          35,
          32.35294117647059,
          32.5,
          30,
          36.25,
          26.25,
          3.75,
          0,
          7.5,
          18.75,
          17.5,
          13.75,
          28.75,
          42.5,
          37.5,
          40.90909090909091,
          26.25,
          37.5,
          15,
          16.666666666666668,
          7.5,
          21.25
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "bargap": 0.35,
        "bargroupgap": 0,
        "barmode": "group",
        "height": 600,
        "legend": {
         "title": {
          "text": "<b>Task</b>"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1100,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "<b>LLM Engine</b>"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          105
         ],
         "ticksuffix": "%",
         "title": {
          "text": "<b>Performance</b>"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "aggregate = (\n",
    "    result_df.groupby([\"agent_name\", \"task\"])[[\"aggregate_score\"]].mean().reset_index()\n",
    ")\n",
    "aggregate[\"agent_name\"] = aggregate[\"agent_name\"].map(\n",
    "    {\n",
    "        \"react_code_llama3-70b_06-may\": \"Llama3-70B-Instruct Code\",\n",
    "        \"react_json_llama3-70b_06-may\": \"Llama3-70B-Instruct JSON\",\n",
    "        \"react_code_llama3-70b_15-may\": \"Llama3-70B-Instruct Code 2\",\n",
    "        \"react_json_llama3-70b_15-may\": \"Llama3-70B-Instruct JSON 2\",\n",
    "        \"react_code_gpt4_06-may\": \"GPT-4-Turbo Code\",\n",
    "        \"react_json_gpt4_06-may\": \"GPT-4-Turbo JSON\",\n",
    "        # \"langchain_GPT-4_06-may\": \"Langchain: GPT4\",\n",
    "        # \"langchain-llama3-70B_06-may\": \"Langchain: Llama3-70B-Instruct\",\n",
    "        # \"react_json_command_r_06-may\": \"Command R+ JSON\",\n",
    "        # \"react_code_command_r_06-may\": \"Command R+ Code\",\n",
    "        # \"react_json_zephyr_large_06-may\": \"Zephyr-141B JSON\",\n",
    "        # \"react_code_zephyr_large_06-may\": \"Zephyr-141B Code\",\n",
    "        \"react_code_mixtral_8x7_06-may\": \"Mixtral-8x7B Code\",\n",
    "        \"react_json_mixtral_8x7_06-may\": \"Mixtral-8x7B JSON\",\n",
    "    }\n",
    ")\n",
    "aggregate = aggregate.sort_values([\"agent_name\", \"task\"], ascending=False)\n",
    "fig = px.bar(\n",
    "    aggregate,\n",
    "    x=\"agent_name\",\n",
    "    y=\"aggregate_score\",\n",
    "    color=\"task\",\n",
    "    labels={\n",
    "        \"agent_name\": \"<b>LLM Engine</b>\",\n",
    "        \"task\": \"<b>Task</b>\",\n",
    "        \"aggregate_score\": \"<b>Performance</b>\",\n",
    "        \"eval_score_GPT4\": \"<b>Score</b>\",\n",
    "        \"OS: Mixtral-8x7B Code\": \"<b>Mixtral-8x7B Code</b>\",\n",
    "    },\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=len(aggregate[\"agent_name\"].unique()) * 100 + 200,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    bargap=0.35,\n",
    "    bargroupgap=0.0,\n",
    "    yaxis_range=[0, 105],\n",
    ")\n",
    "fig.update_traces(texttemplate=\"%{y:.0f}\", textposition=\"outside\")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.write_image(\"aggregate_score.png\", scale=3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
